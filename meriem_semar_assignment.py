# -*- coding: utf-8 -*-
"""Meriem Semar assignment

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BD-JdlWwgFxnzT2nVBjH3dxalSIaAAUJ
"""

#The objective of this exercise is to identify which ZIP codes have experienced the highest/lowest volatility of prices in the last 5 years. In addition, you must also identify which are the main drivers of the volatility for the year 2022.
#Note: volatility means the variation over time of the rent price.

#Data available
#â€¢	Zillow Observed Rent Index (ZORI) - 'RegionID' is ZIP (ZIP is equivalent to postal code in Canada)
#â€¢	Zillow Home Value Index (ZHVI) - 'RegionID' is ZIP
#â€¢	Economic and demographic data for 2022 from Altus Group
#Successful completion of this exercise would do the following:
#1.	Create a list of the top 10 most volatile ZIP codes
#2.	Create a list of the top 10 less volatile ZIP
#3.	Explain what contributes most to the ZIP code being most/less volatile for the year 2022
#4.	Use data science or statistical best practices to build confidence in the validity of (1) and (2)
#5.	Clearly document, organize and explain the building blocks to generate (1) and (2)

#I am importing the necessary libraries to read the data, manipulate it and do a regression as well
import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt
import statsmodels.formula.api as smf

#I am connecting to my google drive to be able to import the necessary datasets
from google.colab import drive
drive.mount('/content/drive')

#Here I am importing the necessary files with the data I need for this exercise. I import it using Pandas and then check that it is the right data by using the head() command
df_zori = pd.read_csv('/content/drive/MyDrive/python data/Zip_zori_sm_sa_month.csv')
df_zori.head()

#same as previous cell
df_zhvi = pd.read_csv('/content/drive/MyDrive/python data/Zip_zhvi_sm_sa_month.csv')
df_zhvi.head()

#same as cell previous cell
df_eco_demo = pd.read_csv('/content/drive/MyDrive/python data/df_eco_demo_2022 (1).csv')
df_eco_demo.head()

"""So for this exercise, I chose to merge Zori and Zhvi on RegionID to be able to calculate the volatility of rent prices (so zori dataset) but also taking into account the house price index. I am firstly restricting each dataset (zori and zhvi) from 2017 to 2022 and then merging both of them to calculate volatility using statistical method. I had thought of calculating volatility for Zori then calculating it for Zhvi separately but it didn't make sense to me to find the volatility according to home value indexes."""

# Here I am restricting values from the data set from 2017 to 2022 (5 most recent years) for zori to be able to calculate volatility of rent prices
df_zori.columns = pd.to_datetime(df_zori.columns, errors='ignore')
#I am selecting the start and end dates
start_date = '31/01/2017'
end_date = '31/12/2022'
#selecting the columns of dates in between start and end date
selected_columns = df_zori.loc[:, start_date:end_date]
#dropping non number values
selected_columns.dropna(inplace=True)
#seeing how it looks if it has been selected well
print(selected_columns)

# I now need to erase the date columns from the original ZORI dataset to use the new selected columns
date_columns = [col for col in df_zori.columns if any(char.isdigit() for char in col)]
#dropping columns
df_zori_cleaned = df_zori.drop(columns=date_columns)
#printing to see how it looks
print(df_zori_cleaned)

# I am combining the restricted columns along the columns in the just cleaned zori dataset
df_combined = pd.concat([df_zori_cleaned, selected_columns], axis=1)
#printing
print(df_combined)

# Here I am restricting values from the data set from 2017 to 2022 (5 most recent years) for zhvi
df_zhvi.columns = pd.to_datetime(df_zhvi.columns, errors='ignore')
#same as steps from zori above
start_date = '31/01/2017'
end_date = '31/12/2022'
selected_columns2 = df_zhvi.loc[:, start_date:end_date]
selected_columns2.dropna(inplace=True)
print(selected_columns2)

#I am erasing the date columns from the original ZHVI dataset to use the new selected columns
date_columns = [col for col in df_zhvi.columns if any(char.isdigit() for char in col)]
df_zhvi_cleaned = df_zhvi.drop(columns=date_columns)
print(df_zhvi_cleaned)

# I am combining the restricted columns along the columns in the just cleaned zhvi dataset
df_combined2 = pd.concat([df_zhvi_cleaned, selected_columns2], axis=1)
print(df_combined2)

#Here I am only keeping RegionID; Date and Rentprice for zori so melting the original combined df down to only 3 columns
vol_data = df_combined.melt(id_vars='RegionID', var_name='Date', value_name='RentPrice')
print(vol_data)

#Here I am only keeping RegionID; Date and Rentprice for zhvi
vol_data2 = df_combined2.melt(id_vars='RegionID', var_name='Date', value_name='RentPrice')
print(vol_data2)

#now I am merging the new zori and zhvi new dataframes on the RegionID column to calculate volatilty. I am adding suffixes _rent and _home_values to differentiate zori (rent prices) and zhvi (home value)
merged_data = vol_data.merge(vol_data2, on='RegionID', suffixes=('_rent', '_home_value'))
print(merged_data)

#This is the calculation of volatility using rentprice according to RegionID
vol_data['RentPrice_rent'] = pd.to_numeric(merged_data['RentPrice_rent'], errors='coerce')
#using standard deviation to calculate volatility
vol_data['Volatility'] = vol_data.groupby('RegionID')['RentPrice_rent'].transform(np.std)
print(vol_data)

#Here I am just dropping the NAs
new_vol_data = vol_data.dropna()
print(new_vol_data)

#This is to calculate the 10 most/least volatile zip codes
most_volatile = new_vol_data.nlargest(10, 'Volatility')['RegionID']
least_volatile = new_vol_data.nsmallest(10, 'Volatility')['RegionID']

#Just printing the zip codes. I can see that the Zip codes here are all the same however, I couldn't find where the mistake was sorry :(
print("The 10 most volatile Zip Codes are:")
print(most_volatile)

#same here
print("The 10 least volatile Zip Codes are:")
print(least_volatile)

"""So after calculating the 10 most and 10 least volatile zip codes, I am going to see which factors affect the most and the least volatility. For this I am going to merge economic and demographic data for 2022 from Altus Group with the merged dataset of zori and zhvi containing volatility. I use this dataset because it shows multiple factors such as Hachman Index or unemployment rate for example. I will need to run a regression to be able to see the p-values of each factors which will show me the significance level. If a factor is significant, it affects volatility."""

#I need to rename the ZIP column from df_eco_demo to RegionID like in the other 2 data files to be able to merge them on RegionID
df_eco_demo.rename(columns={'ZIP': 'RegionID'}, inplace=True)

# Merge the vol data with economic and demographic data on RegionID
merged_data = new_vol_data.merge(df_eco_demo, on='RegionID')
print(merged_data)

#I now need to restrict the merged data only to the year 2022 to check what contributes most to the ZIP code being most/less volatile for the year 2022
merged_data['Date'] = pd.to_datetime(merged_data['Date'], errors='coerce')
#selecting the dates so only limiting to 2022 this time from January to December
start_date = pd.to_datetime('2022-01-31')
end_date = pd.to_datetime('2022-12-31')
restricted_df = merged_data[(merged_data['Date'] >= start_date) & (merged_data['Date'] <= end_date)]
print(restricted_df)

#Here I am running a linear regression of volatility on every other factor shown on the df_econ dataset to see which one contributes the most to volatility
#I am using sklearn to be able to import RinearRegression and to run the regression of volatility on other factors
from sklearn.linear_model import LinearRegression
#I am defining the X (other factors) and y (volatility)
X = restricted_df[['Unemployment rate, 2022', 'Renter affordability, 2022', 'Net migration rate, 2022','Blue Collar Employment Share', 'Tech Employment Share', 'Hachman Index (employment diversity), 2021','Median household net worth, 2022', 'Median renter income, 2022', 'Pct. of all households, Renter, 2022', 'Pct. of all population, BA or higher, 2022', 'Pct. of all population, age 25 to 34, 2022'      ]]
y = restricted_df['Volatility']
#actually running the regression
reg = LinearRegression().fit(X, y)

#here I want to print the summary regression of the model I just ran to check to p-values and coeficients of the factors.
import statsmodels.api as sm
X1 = sm.add_constant(X)
model = sm.OLS(y, X1)
#defining the results to be able to print them and analyse it
results = model.fit()
print(results.summary())

""" First of all, let's look at the p-value for the F test which is used to show the significance of the model. A low p-value (here less than 0.01) suggests that the null hypothesis can be rejected, implying that the model is statistically significant. Therefore, this model is significant. However, the R squared shows  that only 2.8% of the variance in volatility is explained by the model which is extremely low so there is a problem with this model like heteroskedasticity...

 Now, looking at the p-values and coefficients of the factors we can see that Renter affordability, Hachman Index, Median renter income, Pct. of population with BA or higher have the lowest p-values meaning that the factors are statistically significant as we can reject the nul hypothesis and affect volatility.

 However, factors like Unemployment rate, Net migration rate, Blue Collar Employment Share, Tech Employment Share, Median household net worth, Pct. of households that are renters, Pct. of population aged 25 to 34 do not significantly affect volatility according to this model as they have high p-value.


 Now I am going to run an arch test to build confidence in the validity of the calculation of volatility. I use this test as it is helpful to test for heteroskedasticity (when the variance of the errors in a regression model is not constant across all levels of the independent variables) and also as I remember using it in econometrics. I am also plotting the residuals to see how they evolve over time.
"""

#I am installing the Arch model to test if the model has heteroskedasticty
!pip3 install arch
from arch import arch_model

# Extracting the residuals to use in the model
residuals = y - reg.predict(X)

#making sure it is a panda series or numpy array so that it can be fit into the garch model
print(type(residuals))

#Plotting the residuals to see how it looks in terms of volatility
plt.figure(5)
plt.plot(results.resid)
#defining the axis
plt.xlabel('Time')
plt.ylabel('Residuals')
plt.grid(True)
plt.show()

"""We can see that there are a lot of spikes over time which shows rapid fluctuations in returns/ volatility.

"""

# Fit an ARCH model to be able to perform an ARCH test
arch_model1 = arch_model(residuals, vol='Arch', p=1, q=1)
arch_results = arch_model1.fit()

#printing the summary to see how it looks
print(arch_results.summary())

#Performing the ARCH test to check for heteroskedasticity in the data
from statsmodels.stats.diagnostic import het_arch
arch_test = het_arch(residuals)
#extracting the Test stat and p-value from the residuals
print(f'ARCH Test Statistic: {arch_test[0]}')
print(f'ARCH Test p-value: {arch_test[1]}')

"""As the p-value is less than 0.05, we can reject the null hypothesis of homoskedasticity so we have heteroskedasticity and need to use a Garch model to correct for it. So the last step of this exercise is to fit a Garch model to be able to see how it works.

"""

# Here I am fitting a GARCH model to correct heteroskedasticity saw in the arch test
garch_model1 = arch_model(residuals, vol='Garch', p=1, q=1)
garch_results = garch_model1.fit()
#printing the results summary where we can look at p-value; r squared...
print(garch_results.summary())

"""The very low R-squared value indicates that the model does not explain much of the variability in the volatility, suggesting the model might need further refinement or additional variables. The GARCH model shows that the past shocks are the main drivers of current volatility, as indicated by the significant alpha[1] coefficient."""